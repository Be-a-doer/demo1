---
title: "206 Project"
author: "Yifan Wu"
date: "2024-11-30"
format:
  html:
    css: styles.css
    toc: true
    toc-depth: 3
    toc-location: left
    embed-resources: true
    code-line-numbers: true
  pdf:
    toc: true
    toc-depth: 3
    code-line-numbers: true
    fontsize: 10pt
fig-cap-location: margin
editor: visual
jupyter: julia-1.9
---

# Principal Component Analysis(PCA)

## The Introduction of PCA

When understanding feature extraction and processing, problems involving high-dimensional feature vectors are often prone to **dimensional disasters**. As the dimensionality of the dataset increases, the number of samples required for algorithm learning increases exponentially.

PCA is commonly used for exploring and visualizing high-dimensional data-sets; It can also be used for data compression, data pre-processing, etc. PCA can synthesize high-dimensional variables that may have correlation into linearly independent low dimensional variables, known as principal components.

## The Principle of PCA Method

PCA projects data onto a **low dimensional subspace** to achieve dimensionality reduction.

For example, dimensionality reduction in a 2 dimensional data-set is to project the surface into a line, and each sample in the data-set can be represented by a value on the line, without the need for two values on the surface. This achieves the operation of reducing data from 2 dimensional to 1 dimensional.

In general, an n-dimensional data-set can be reduced to a k-k-dimensional subspace through mapping, where k â‰¤ n.

Assuming we have a 2 dimensional data-set as shown in the following figure, each sample in the data-set is represented by two variables.

::: {style="text-align: center; margin: 20px 0;"}
<img src="images/PCA1.png" alt="PCA1" width="600"/>
:::

To reduce the dimensionality of the entire dataset, we must map the points into a line. The two lines in the following figure can be mapped by the dataset. Which line does the sample change the most when mapped to?

::: {style="text-align: center; margin: 20px 0;"}
<img src="images/PCA2.png" alt="PCA2" width="600"/>
:::

Obviously, the change in mapping the sample to the black dashed line is much greater than the change in mapping to the red dotted line. In fact, this black dashed line is the first principal component. The second principal component must be orthogonal to the first principal component, as shown in the following figure:

::: {style="text-align: center; margin: 20px 0;"}
<img src="images/PCA3.png" alt="PCA3" width="600"/>
:::

So we have found the direction of the two principal components.

## R code for PCA
```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(rgl)

# Load wine dataset
# This simulates loading the wine dataset similar to sklearn's load_wine function in Python.
wine_data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", header = FALSE)
colnames(wine_data) <- c("Class", paste0("Feature", 1:13))

# Subset data for PCA (select rows 1:130 and features 9:11)
data_raw <- wine_data[1:130, 10:12]
target_raw <- wine_data[1:130, 1]

# 2.1 Centering the data
# Calculate the mean for each feature and normalize the data by subtracting the mean
data_mean <- colMeans(data_raw)
data_norm <- scale(data_raw, center = data_mean, scale = FALSE)

# 2.2 Compute covariance matrix
# The covariance matrix is calculated using the normalized data.
cov_matrix <- cov(data_norm)

# 2.3 Compute eigenvalues and eigenvectors
# Use eigen decomposition to get eigenvalues and eigenvectors.
eig <- eigen(cov_matrix)
eig_values <- eig$values
eig_vectors <- eig$vectors

# 2.4 Sort eigenvectors by descending eigenvalues
# Order the indices of eigenvalues in descending order.
new_index <- order(eig_values, decreasing = TRUE)
matrix_w <- eig_vectors[, new_index]

# 2.5 Compute contribution rates
# Compute the proportion of variance explained by each eigenvalue.
contribution <- eig_values[new_index] / sum(eig_values)
print(contribution)

# 2.6 Project data onto the new subspace and retain M dimensions
# For visualization, retain the first two principal components.
M <- 2
PCA_data <- as.matrix(data_norm) %*% matrix_w
PCA_data <- PCA_data[, 1:M]

# 4.1 Visualization of the data before PCA
# Use rgl for 3D scatter plot visualization.
colors <- c("blue", "red", "green")
shapes <- c(16, 17, 15)
open3d()
plot3d(data_raw[, 1], data_raw[, 2], data_raw[, 3], col = colors[target_raw], type = "s", size = 0.5)
title3d("Samples before PCA", xlab = "Feature 1", ylab = "Feature 2", zlab = "Feature 3")
legend3d("topright", legend = c("class1", "class2", "class3"), col = colors, pch = shapes)

# 4.2 Visualization of the data after PCA
# Use ggplot2 for 2D scatter plot visualization after PCA.
pca_df <- data.frame(PCA_data, Class = as.factor(target_raw))
colnames(pca_df) <- c("dim_1", "dim_2", "Class")

plot2d <- ggplot(pca_df, aes(x = dim_1, y = dim_2, color = Class, shape = Class)) +
  geom_point(size = 2, alpha = 0.5) +
  scale_color_manual(values = colors) +
  scale_shape_manual(values = shapes) +
  labs(title = "Samples after PCA", x = "dim_1", y = "dim_2") +
  theme_minimal()
print(plot2d)
```